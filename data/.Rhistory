group_by(line) %>%
summarise(count5 = n())
review3 <- review1 %>%
full_join(x1, by = "line") %>%
full_join(x2, by = "line") %>%
full_join(x3, by = "line") %>%
full_join(x4, by = "line") %>%
full_join(x5, by = "line")
review3$count1[which(is.na(review3$count1))] = 0
review3$count2[which(is.na(review3$count2))] = 0
review3$count3[which(is.na(review3$count3))] = 0
review3$count4[which(is.na(review3$count4))] = 0
review3$count5[which(is.na(review3$count5))] = 0
new <- review3 %>%
select(count1:count5)
######
new = data.frame(new)
colnames(new) = c('s1','s2','s3','s4','s5')
review$date = apply(as.data.frame(review$date),1,FUN=year)
id1 = which(review$longitude< -100)
id2 = which(review$longitude> -100 & review$longitude< -25)
id3 = which(review$longitude> -25)
loc = rep(0,nrow(review))
loc1 = loc
loc1[id1] = 1
loc2 = loc
loc2[id2] = 1
loc3 = loc
loc3[id3] = 1
review = review %>% mutate(loc1=loc1,loc2=loc2,loc3=loc3)
# review = read_csv('test_final.csv')
review2 = cbind(review,new)
write_csv(review2,'train_final3.csv')
library(tidyverse)
library(tidytext)
library(Matrix)
library(topicmodels)
library(glmnet)
data_ori = read_csv("../../data/random100000w.csv")
data_df = data_frame(line = 1:nrow(data_ori),
text = data_ori$text,
stars = data_ori$stars)
data_tidy <- data_df %>%
unnest_tokens(word, text)
which(data_tidy$word=="nothappier")
data_tidy$line[which(data_tidy$word=="nothappier")]
index = data_tidy$line[which(data_tidy$word=="nothappier")]
data_df$text[630]
library(tidyverse)
library(tidytext)
library(Matrix)
install.packages("tidytext")
library(Matrix)
library(topicmodels)
install.packages("topicmodels")
library(glmnet)
data_ori = read_csv("../../data/random100000w.csv")
data_df = data_frame(line = 1:nrow(data_ori),
text = data_ori$text,
stars = data_ori$stars)
data_df
data_df1 = data_df[1:500000,]
t1 = Sys.time()
data_tidy1 <- data_df1 %>%
unnest_tokens(word, text)
library(tidytext)
t1 = Sys.time()
data_tidy1 <- data_df1 %>%
unnest_tokens(word, text)
t2 = Sys.time()
t2-t1
t1 = Sys.time()
data_tidy2 <- data_df2 %>%
unnest_tokens(word, text)
data_df2 = data_df[500001:1000000,]
data_df3 = data_df[1000001:nrow(data_ori),]
t1 = Sys.time()
data_tidy2 <- data_df2 %>%
unnest_tokens(word, text)
t2 = Sys.time()
t2-t1
data_df1 = data_df[1:500000,]
data_df2 = data_df[500001:1000000,]
data_df3 = data_df[1000001:nrow(data_ori),]
t1 = Sys.time()
data_df2
dim(data_df)
data_ori
data_ori = read_csv("../../data/random100000w.csv")
data_df = data_frame(line = 1:nrow(data_ori),
text = data_ori$text,
stars = data_ori$stars)
t1 = Sys.time()
data_tidy <- data_df %>%
unnest_tokens(word, text)
t2 = Sys.time()
t2-t1
data_tidy
data_df
data_neg <- data_tidy %>%
filter(stars == 1)
data_neg
data_neg <- data_df %>%
filter(stars == 1)
data_neg
data_neg$text[1]
data_ori = read_csv("../../data/random100000.csv")
data_df = data_frame(line = 1:nrow(data_ori),
text = data_ori$text,
stars = data_ori$stars)
data_neg <- data_df %>%
filter(stars == 1)
data_neg$text[1]
data_neg$text[2]
data_neg$text[3]
library(tidyverse)
library(tidytext)
library(Matrix)
library(topicmodels)
library(glmnet)
data_ori = read_csv("../../data/random100000.csv")
data_df = data_frame(line = 1:nrow(data_ori),
text = data_ori$text,
stars = data_ori$stars)
data_df
data_df %>%
filter(stars==1)
data_df = data_frame(line = 1:nrow(data_ori),
text = data_ori$text,
stars = data_ori$stars)
t1 = Sys.time()
data_tidy <- data_df %>%
unnest_tokens(word, text)
t2 = Sys.time()
t2-t1
data_tidy
data_tidy %>%
filter(word == "disrespectful")
linshi <- data_tidy %>%
filter(word == "disrespectful")
levels(linshi$line)
unique(linshi$line)
length(unique(linshi$line))
linshi <- data_tidy %>%
filter(word == "is")
length(unique(linshi$line))
data_df_linshi <- data_df %>%
filter(stars==1)
linshi <- data_df_linshi %>%
filter(word == "is")
data_df_linshi <- data_df %>%
filter(stars==1)
linshi <- data_df_linshi %>%
filter(word == "is")
length(unique(linshi$line))
data_df_linshi <- data_df %>%
filter(stars==1)
linshi <- data_df_linshi %>%
filter(word == "is")
data_df_linshi %>%
filter(word == "is")
data_df_linshi
data_df_linshi <- data_tidy %>%
filter(stars==1)
linshi <- data_df_linshi %>%
filter(word == "is")
length(unique(linshi$line))
linshi <- data_df_linshi %>%
filter(word == "disrespectful")
length(unique(linshi$line))
library(tidyverse)
library(tidytext)
library(Matrix)
library(topicmodels)
library(glmnet)
data_ori = read_csv("../../data/random100000w.csv")
data_df = data_frame(line = 1:nrow(data_ori),
text = data_ori$text,
stars = data_ori$stars)
data_df
library(tidyverse)
library(tidytext)
library(Matrix)
library(topicmodels)
library(glmnet)
data_ori = read_csv("../../data/random100000.csv")
dim(data_ori)
data_df = data_frame(line = 1:nrow(data_ori),
text = data_ori$text,
stars = data_ori$stars)
data_df1 = data_df
t1 = Sys.time()
data_tidy1 <- data_df1 %>%
unnest_tokens(word, text)
t2 = Sys.time()
t2-t1
t1 = Sys.time()
data_tidy11 <- data_tidy1 %>%
distinct(line, word, stars)
t2 = Sys.time()
t2-t1
data_tidy <- data_tidy11
data_tidy2 <- data_tidy %>%
group_by(stars, word) %>%
summarise(count = n())
data_tidy2
data_tidy2_5 <- data_tidy2 %>%
group_by(word) %>%
summarise(count2_5 = sum(count)) %>%
filter(count2_5 > 100)
data_tidy2_5
major <- data_tidy2_5 %>%
select(word)
major
data_tidy3 <- data_tidy2 %>%
inner_join(major, by = "word") %>%
spread(stars, count, fill = 0)
data_tidy3
data_df = data_frame(line = 1:nrow(data_ori),
text = data_ori$text,
stars = data_ori$stars)
t1 = Sys.time()
data_tidy <- data_df %>%
unnest_tokens(word, text)
t2 = Sys.time()
t2-t1
t1 = Sys.time()
data_tidy_dis <- data_tidy %>%
distinct(line, word, stars)
t2 = Sys.time()
t2-t1
data_tidy_unique <- data_tidy %>%
distinct(line, word, stars)
data_tidy_group <- data_tidy_unique %>%
group_by(stars, word) %>%
summarise(count = n())
data_tidy_group
data_tidy_unique
data_tidy_group %>%
group_by(word)
data_tidy_group_100 <- data_tidy_group %>%
group_by(word) %>%
summarise(count2_5 = sum(count)) %>%
filter(count2_5 > 100)
data_tidy_group_100
major_word <- data_tidy_group_100 %>%
select(word)
major_word
data_tidy_group_100 %>%
inner_join(major, by = "word") %>%
spread(stars, count, fill = 0)
data_tidy_group_100 %>%
inner_join(major_word, by = "word")
data_tidy3 <- data_tidy_group %>%
inner_join(major_word, by = "word") %>%
spread(stars, count, fill = 0)
data_tidy3
star_count <- data_df %>%
group_by(stars) %>%
summarise(count = n())
star_count
mycount = star_count$count
myprop = function(data_row, counts){
data2 = as.numeric(data_row[2:6])
xx = numeric(5)
for(i in 1:5){
if(counts[i] == 0){
xx[i] = 0
}else if(sum(data2[-i]) == 0){
xx[i] = 100
}else if(sum(counts[-i]) == 0){
xx[i] = 100
}else{
y1 = data2[i]/counts[i]
y2 = sum(data2[-i])/sum(counts[-i])
xx[i] = y1/y2
}
}
return(xx)
}
t1 = Sys.time()
result = as.tibble(t(apply(data_tidy3, 1, myprop, counts = mycount)))
t2 = Sys.time()
t2-t1
result
result$word = data_tidy3$word
result <- result %>%
select(word, V1:V5)
result
filename = paste("feature4jianmin_all.csv")
write_csv(result, filename)
getwd()
setwd("/Users/ivanlee/Documents/GitHub/STAT-5665-project/data")
filename = paste("../data/feature4jianmin_all.csv")
write_csv(result, filename)
result %>%
arrange(desc(V1))
data_tidy3[which(data_tidy3$word %in% "refund"),][2:6]/mycount
result[which(result$word %in% "and"),]
dim(result)
data_tidy
length(unique(data_tidy$word))
?first
?top_n
result %>%
arrange(desc(V1)) %>%
top_n(100)
result
result %>%
arrange(desc(V1))
result %>%
arrange(desc(V1)) %>%
top_n(100, V1)
S1 <- result %>%
arrange(desc(V1)) %>%
top_n(100, V1) %>%
select(word)
S1
S2 <- result %>%
arrange(desc(V2)) %>%
top_n(100, V2) %>%
select(word)
S1 <- result %>%
arrange(desc(V1)) %>%
top_n(1000, V1) %>%
select(word)
S2 <- result %>%
arrange(desc(V2)) %>%
top_n(1000, V2) %>%
select(word)
S3 <- result %>%
arrange(desc(V3)) %>%
top_n(1000, V3) %>%
select(word)
S4 <- result %>%
arrange(desc(V4)) %>%
top_n(1000, V4) %>%
select(word)
S5 <- result %>%
arrange(desc(V5)) %>%
top_n(1000, V5) %>%
select(word)
S4 <- result %>%
arrange(desc(V4)) %>%
top_n(1000, V4) %>%
select(word)
S5 <- result %>%
arrange(desc(V5)) %>%
top_n(1000, V5) %>%
select(word)
S1
S2
S3
S4
S5
S1 <- result %>%
arrange(desc(V1)) %>%
top_n(1000, V1) %>%
select(word, V1)
S1
S1 <- result %>%
arrange(desc(V1)) %>%
top_n(1000, V1) %>%
select(word, V1)
S2 <- result %>%
arrange(desc(V2)) %>%
top_n(1000, V2) %>%
select(word, V2)
S3 <- result %>%
arrange(desc(V3)) %>%
top_n(1000, V3) %>%
select(word, V3)
S4 <- result %>%
arrange(desc(V4)) %>%
top_n(1000, V4) %>%
select(word, V4)
S5 <- result %>%
arrange(desc(V5)) %>%
top_n(1000, V5) %>%
select(word, V5)
S5
data_ori <- read_csv("../../data/random100000.csv")
data_ori <- read_csv("../data/random100000.csv")
pos <- read_csv("../data/feature4jianmin_all.csv")
S1 <- result %>%
arrange(desc(V1)) %>%
top_n(1000, V1) %>%
select(word, V1)
result <- read_csv("../data/feature4jianmin_all.csv")
S1 <- result %>%
arrange(desc(V1)) %>%
top_n(1000, V1) %>%
select(word, V1)
S2 <- result %>%
arrange(desc(V2)) %>%
top_n(1000, V2) %>%
select(word, V2)
S3 <- result %>%
arrange(desc(V3)) %>%
top_n(1000, V3) %>%
select(word, V3)
S4 <- result %>%
arrange(desc(V4)) %>%
top_n(1000, V4) %>%
select(word, V4)
S5 <- result %>%
arrange(desc(V5)) %>%
top_n(1000, V5) %>%
select(word, V5)
S1
S2
S3
S4
S5
data_ori
data_df = data_frame(line = 1:nrow(data_ori),
text = data_ori$text,
stars = data_ori$stars)
t1 = Sys.time()
data_tidy <- data_df %>%
unnest_tokens(word, text)
t2 = Sys.time()
t2-t1
t1 = Sys.time()
data_tidy_unique <- data_tidy %>%
distinct(line, word, stars)
t2 = Sys.time()
t2-t1
data_tidy_unique
N <- nrow(data_ori)
new_data <- tibble(line = c(1:N),
S1 = rep(0,N),
S2 = rep(0,N),
S3 = rep(0,N),
S4 = rep(0,N),
S5 = rep(0,N))
new_data
data_tidy_unique
dim(data_tidy_unique)
i=1000
row = data_tidy_unique[i,]
row
data_tidy
?
left_join
S1
data_tidy %>%
left_join(S1, by = word)
data_tidy %>%
left_join(S1, by = "word")
data_tidy %>%
right_join(S1, by = "word")
data_tidy %>%
right_join(S1, by = "word") %>%
group_by(line) %>%
summarise(count = n())
linshi <- data_tidy %>%
right_join(S1, by = "word") %>%
group_by(line) %>%
summarise(count = n())
new_data[linshi$line,]
new_data[linshi$line,2] <- linshi$count
new_data
new_data <- tibble(line = c(1:N),
S1 = rep(0,N),
S2 = rep(0,N),
S3 = rep(0,N),
S4 = rep(0,N),
S5 = rep(0,N))
linshi <- data_tidy %>%
right_join(S1, by = "word") %>%
group_by(line) %>%
summarise(count = n())
t1 = Sys.time()
linshi <- data_tidy %>%
right_join(S1, by = "word") %>%
group_by(line) %>%
summarise(count = n())
new_data[linshi$line,2] <- linshi$count
linshi <- data_tidy %>%
right_join(S2, by = "word") %>%
group_by(line) %>%
summarise(count = n())
new_data[linshi$line,3] <- linshi$count
linshi <- data_tidy %>%
right_join(S3, by = "word") %>%
group_by(line) %>%
summarise(count = n())
new_data[linshi$line,4] <- linshi$count
linshi <- data_tidy %>%
right_join(S4, by = "word") %>%
group_by(line) %>%
summarise(count = n())
new_data[linshi$line,5] <- linshi$count
linshi <- data_tidy %>%
right_join(S5, by = "word") %>%
group_by(line) %>%
summarise(count = n())
new_data[linshi$line,6] <- linshi$count
t2 = Sys.time()
t2-t1
new_data
new_data$stars <- data_ori$stars
new_data
?apply
linshi <- apply(new_data[,2:6], 1, sum)
linshi
sum(linshi==0)
filename = paste("../data/data_final.csv")
write_csv(new_data, filename)
filename = paste("../data/pos_neg.csv")
write_csv(result, filename)
